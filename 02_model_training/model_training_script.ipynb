{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install imblearn\n",
    "# !pip3 install deployed\n",
    "# !pip3 install xgboost\n",
    "# !pip3 install plotly\n",
    "# !pip3 install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Import Python Modules\n",
    "####################################\n",
    "# General Purpose Modules\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Processing Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML Modules\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score,precision_score, recall_score \n",
    "\n",
    "\n",
    "# Custom modules\n",
    "from dataset_schema_dict import dataset_schema\n",
    "\n",
    "seed = 99\n",
    "_seed = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('research_data_7days_above_20_2023.csv', index_col=False)\n",
    "data = data.dropna()\n",
    "\n",
    "data['target_d3_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.03*data['t0']), 1, 0)))))\n",
    "\n",
    "data['target_d5_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.03*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.03*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.03*data['t0']), 1, 0)))))))))\n",
    "\n",
    "data['target_d7_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.03*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.03*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.03*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t6']> (1.03*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t7']> (1.03*data['t0']), 1, 0)))))))))))))\n",
    "\n",
    "data['target_d3_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.05*data['t0']), 1, 0)))))\n",
    "\n",
    "data['target_d5_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.05*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.05*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.05*data['t0']), 1, 0)))))))))\n",
    "\n",
    "\n",
    "data['target_d7_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.05*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.05*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.05*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t6']> (1.05*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t7']> (1.05*data['t0']), 1, 0)))))))))))))\n",
    "\n",
    "\n",
    "data['target_d3_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.07*data['t0']), 1, 0)))))\n",
    "\n",
    "data['target_d5_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.07*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.07*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.07*data['t0']), 1, 0)))))))))\n",
    "\n",
    "\n",
    "data['target_d7_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t3']> (1.07*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t4']> (1.07*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t5']> (1.07*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t6']> (1.07*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
    "               np.where(data['t7']> (1.07*data['t0']), 1, 0)))))))))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "04    15121\n",
       "03    14979\n",
       "12    14916\n",
       "11    14422\n",
       "01    12808\n",
       "02    11977\n",
       "10     3696\n",
       "05     2215\n",
       "Name: month, dtype: int64"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['day_name'] = pd.to_datetime(data['date'], format='%d/%m/%Y').dt.day_name()\n",
    "data['month'] = pd.to_datetime(data['date'], format='%d/%m/%Y').dt.strftime('%m')\n",
    "data['month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    87\n",
       "float64      8\n",
       "object       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_columns = data.select_dtypes(include=['int64','int32']).columns\n",
    "float_columns = data.select_dtypes(include=['float']).columns\n",
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "data[integer_columns] = data[integer_columns].astype('category')\n",
    "\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_d3_p3\n",
      "0    0.727905\n",
      "1    0.272095\n",
      "Name: target_d3_p3, dtype: float64\n",
      "target_d5_p3\n",
      "0    0.652606\n",
      "1    0.347394\n",
      "Name: target_d5_p3, dtype: float64\n",
      "target_d7_p3\n",
      "0    0.609215\n",
      "1    0.390785\n",
      "Name: target_d7_p3, dtype: float64\n",
      "target_d3_p5\n",
      "0    0.821677\n",
      "1    0.178323\n",
      "Name: target_d3_p5, dtype: float64\n",
      "target_d5_p5\n",
      "0    0.75289\n",
      "1    0.24711\n",
      "Name: target_d5_p5, dtype: float64\n",
      "target_d7_p5\n",
      "0    0.708545\n",
      "1    0.291455\n",
      "Name: target_d7_p5, dtype: float64\n",
      "target_d3_p7\n",
      "0    0.877172\n",
      "1    0.122828\n",
      "Name: target_d3_p7, dtype: float64\n",
      "target_d5_p7\n",
      "0    0.818836\n",
      "1    0.181164\n",
      "Name: target_d5_p7, dtype: float64\n",
      "target_d7_p7\n",
      "0    0.777232\n",
      "1    0.222768\n",
      "Name: target_d7_p7, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_list = [col for col in data if col.startswith('target')]\n",
    "\n",
    "for col in target_list:\n",
    "    print (col)\n",
    "    print(data[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90105</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.790</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90106</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90107</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90108</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90109</th>\n",
       "      <td>1.86</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.880</td>\n",
       "      <td>1.920</td>\n",
       "      <td>1.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26270 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         t0     t1     t2     t3     t4     t5     t6     t7\n",
       "5      0.81  0.825  0.810  0.820  0.840  0.845  0.845  0.860\n",
       "7      0.81  0.820  0.840  0.845  0.845  0.860  0.860  0.875\n",
       "8      0.82  0.840  0.845  0.845  0.860  0.860  0.875  0.870\n",
       "25     0.85  0.860  0.840  0.840  0.840  0.855  0.870  0.930\n",
       "26     0.86  0.840  0.840  0.840  0.855  0.870  0.930  0.950\n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...\n",
       "90105  1.79  1.790  1.800  1.800  1.860  1.900  1.920  1.920\n",
       "90106  1.79  1.800  1.800  1.860  1.900  1.920  1.920  1.980\n",
       "90107  1.80  1.800  1.860  1.900  1.920  1.920  1.980  1.880\n",
       "90108  1.80  1.860  1.900  1.920  1.920  1.980  1.880  1.920\n",
       "90109  1.86  1.900  1.920  1.920  1.980  1.880  1.920  1.900\n",
       "\n",
       "[26270 rows x 8 columns]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario = 'd7_p5'\n",
    "data[['t0', 't1','t2','t3','t4','t5','t6','t7']][data[f'target_{scenario}'] == 1]\n",
    "#data[['date','t0', 't1','t2','t3','t4','t5','t6','t7']][data['Name'] == 'GRANFLO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90134, 99)\n",
      "Split OK? : True\n"
     ]
    }
   ],
   "source": [
    "##Define Train OOT (random split)\n",
    "\n",
    "df1 = data.drop(columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\n",
    "df1.set_index(['Name', 'date'], inplace=True)\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.8, seed=seed):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    train = df.loc[perm[:train_end]]\n",
    "    oot = df.loc[perm[train_end:]]\n",
    "    return train, oot\n",
    "\n",
    "train, oot = train_validate_test_split(df1)\n",
    "\n",
    "print (data.shape)\n",
    "print (f'Split OK? : {len(train) + len(oot)== len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "0:00:00.002678\n",
      "\n",
      "encoding categorical column:\n",
      "20-day-high\n",
      "\n",
      "saved ordinal encoder for 20-day-high at\n",
      "./python_model_objects/uplift/ordEnc_20-day-high.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "3-ducks\n",
      "\n",
      "saved ordinal encoder for 3-ducks at\n",
      "./python_model_objects/uplift/ordEnc_3-ducks.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "52-week-high\n",
      "\n",
      "saved ordinal encoder for 52-week-high at\n",
      "./python_model_objects/uplift/ordEnc_52-week-high.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "52-week-low\n",
      "\n",
      "saved ordinal encoder for 52-week-low at\n",
      "./python_model_objects/uplift/ordEnc_52-week-low.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "above-ma50\n",
      "\n",
      "saved ordinal encoder for above-ma50 at\n",
      "./python_model_objects/uplift/ordEnc_above-ma50.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "all-time-high\n",
      "\n",
      "saved ordinal encoder for all-time-high at\n",
      "./python_model_objects/uplift/ordEnc_all-time-high.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "atr\n",
      "\n",
      "saved ordinal encoder for atr at\n",
      "./python_model_objects/uplift/ordEnc_atr.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "blue-chip-uptrend\n",
      "\n",
      "saved ordinal encoder for blue-chip-uptrend at\n",
      "./python_model_objects/uplift/ordEnc_blue-chip-uptrend.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "bollinger-band-breakout\n",
      "\n",
      "saved ordinal encoder for bollinger-band-breakout at\n",
      "./python_model_objects/uplift/ordEnc_bollinger-band-breakout.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "bollinger-band-oversold\n",
      "\n",
      "saved ordinal encoder for bollinger-band-oversold at\n",
      "./python_model_objects/uplift/ordEnc_bollinger-band-oversold.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "bollinger-band-squeeze\n",
      "\n",
      "saved ordinal encoder for bollinger-band-squeeze at\n",
      "./python_model_objects/uplift/ordEnc_bollinger-band-squeeze.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "bollinger-band-swing\n",
      "\n",
      "saved ordinal encoder for bollinger-band-swing at\n",
      "./python_model_objects/uplift/ordEnc_bollinger-band-swing.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "bullish-candlestick\n",
      "\n",
      "saved ordinal encoder for bullish-candlestick at\n",
      "./python_model_objects/uplift/ordEnc_bullish-candlestick.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "candle-4r1g\n",
      "\n",
      "saved ordinal encoder for candle-4r1g at\n",
      "./python_model_objects/uplift/ordEnc_candle-4r1g.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "cci-cross\n",
      "\n",
      "saved ordinal encoder for cci-cross at\n",
      "./python_model_objects/uplift/ordEnc_cci-cross.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "chaikin-money-flow-cmf\n",
      "\n",
      "saved ordinal encoder for chaikin-money-flow-cmf at\n",
      "./python_model_objects/uplift/ordEnc_chaikin-money-flow-cmf.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ema5-cross-sma10\n",
      "\n",
      "saved ordinal encoder for ema5-cross-sma10 at\n",
      "./python_model_objects/uplift/ordEnc_ema5-cross-sma10.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ema5-cross-sma9\n",
      "\n",
      "saved ordinal encoder for ema5-cross-sma9 at\n",
      "./python_model_objects/uplift/ordEnc_ema5-cross-sma9.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ema7-cross-sma200\n",
      "\n",
      "saved ordinal encoder for ema7-cross-sma200 at\n",
      "./python_model_objects/uplift/ordEnc_ema7-cross-sma200.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "erp5-momentum\n",
      "\n",
      "saved ordinal encoder for erp5-momentum at\n",
      "./python_model_objects/uplift/ordEnc_erp5-momentum.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "fbo-almost\n",
      "\n",
      "saved ordinal encoder for fbo-almost at\n",
      "./python_model_objects/uplift/ordEnc_fbo-almost.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "fbo-recent\n",
      "\n",
      "saved ordinal encoder for fbo-recent at\n",
      "./python_model_objects/uplift/ordEnc_fbo-recent.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "gann-square-of-nine\n",
      "\n",
      "saved ordinal encoder for gann-square-of-nine at\n",
      "./python_model_objects/uplift/ordEnc_gann-square-of-nine.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "gap-up\n",
      "\n",
      "saved ordinal encoder for gap-up at\n",
      "./python_model_objects/uplift/ordEnc_gap-up.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "heikin-ashi-4g1r\n",
      "\n",
      "saved ordinal encoder for heikin-ashi-4g1r at\n",
      "./python_model_objects/uplift/ordEnc_heikin-ashi-4g1r.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "heikin-ashi-4r1g\n",
      "\n",
      "saved ordinal encoder for heikin-ashi-4r1g at\n",
      "./python_model_objects/uplift/ordEnc_heikin-ashi-4r1g.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "heikin-ashi-strong-buying-pressure\n",
      "\n",
      "saved ordinal encoder for heikin-ashi-strong-buying-pressure at\n",
      "./python_model_objects/uplift/ordEnc_heikin-ashi-strong-buying-pressure.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "heikin-ashi-strong-selling-pressure\n",
      "\n",
      "saved ordinal encoder for heikin-ashi-strong-selling-pressure at\n",
      "./python_model_objects/uplift/ordEnc_heikin-ashi-strong-selling-pressure.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-above-kumo\n",
      "\n",
      "saved ordinal encoder for ichimoku-above-kumo at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-above-kumo.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-bearish-reversal\n",
      "\n",
      "saved ordinal encoder for ichimoku-bearish-reversal at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-bearish-reversal.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-bullish-reversal\n",
      "\n",
      "saved ordinal encoder for ichimoku-bullish-reversal at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-bullish-reversal.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-chikou-span-cross\n",
      "\n",
      "saved ordinal encoder for ichimoku-chikou-span-cross at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-chikou-span-cross.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-kijun-sen-cross\n",
      "\n",
      "saved ordinal encoder for ichimoku-kijun-sen-cross at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-kijun-sen-cross.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-kumo-breakout\n",
      "\n",
      "saved ordinal encoder for ichimoku-kumo-breakout at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-kumo-breakout.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ichimoku-kumo-twist\n",
      "\n",
      "saved ordinal encoder for ichimoku-kumo-twist at\n",
      "./python_model_objects/uplift/ordEnc_ichimoku-kumo-twist.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ikh-ha-strong-buying-pressure\n",
      "\n",
      "saved ordinal encoder for ikh-ha-strong-buying-pressure at\n",
      "./python_model_objects/uplift/ordEnc_ikh-ha-strong-buying-pressure.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ikh-ha-strong-selling-pressure\n",
      "\n",
      "saved ordinal encoder for ikh-ha-strong-selling-pressure at\n",
      "./python_model_objects/uplift/ordEnc_ikh-ha-strong-selling-pressure.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "increasing-ttm-ichimoku\n",
      "\n",
      "saved ordinal encoder for increasing-ttm-ichimoku at\n",
      "./python_model_objects/uplift/ordEnc_increasing-ttm-ichimoku.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "kelia-ma-cross\n",
      "\n",
      "saved ordinal encoder for kelia-ma-cross at\n",
      "./python_model_objects/uplift/ordEnc_kelia-ma-cross.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "long-term-sideway\n",
      "\n",
      "saved ordinal encoder for long-term-sideway at\n",
      "./python_model_objects/uplift/ordEnc_long-term-sideway.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "lower-high-lower-low\n",
      "\n",
      "saved ordinal encoder for lower-high-lower-low at\n",
      "./python_model_objects/uplift/ordEnc_lower-high-lower-low.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma-as-support\n",
      "\n",
      "saved ordinal encoder for ma-as-support at\n",
      "./python_model_objects/uplift/ordEnc_ma-as-support.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma20\n",
      "\n",
      "saved ordinal encoder for ma20 at\n",
      "./python_model_objects/uplift/ordEnc_ma20.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma20-as-support\n",
      "\n",
      "saved ordinal encoder for ma20-as-support at\n",
      "./python_model_objects/uplift/ordEnc_ma20-as-support.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma20-cross-ma50\n",
      "\n",
      "saved ordinal encoder for ma20-cross-ma50 at\n",
      "./python_model_objects/uplift/ordEnc_ma20-cross-ma50.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma200\n",
      "\n",
      "saved ordinal encoder for ma200 at\n",
      "./python_model_objects/uplift/ordEnc_ma200.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma30-cross-ma200\n",
      "\n",
      "saved ordinal encoder for ma30-cross-ma200 at\n",
      "./python_model_objects/uplift/ordEnc_ma30-cross-ma200.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma5\n",
      "\n",
      "saved ordinal encoder for ma5 at\n",
      "./python_model_objects/uplift/ordEnc_ma5.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma50\n",
      "\n",
      "saved ordinal encoder for ma50 at\n",
      "./python_model_objects/uplift/ordEnc_ma50.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "ma7-cross-ma26\n",
      "\n",
      "saved ordinal encoder for ma7-cross-ma26 at\n",
      "./python_model_objects/uplift/ordEnc_ma7-cross-ma26.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "macd-4r1g-above-signal\n",
      "\n",
      "saved ordinal encoder for macd-4r1g-above-signal at\n",
      "./python_model_objects/uplift/ordEnc_macd-4r1g-above-signal.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "macd-above-0\n",
      "\n",
      "saved ordinal encoder for macd-above-0 at\n",
      "./python_model_objects/uplift/ordEnc_macd-above-0.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "macd-cross-0\n",
      "\n",
      "saved ordinal encoder for macd-cross-0 at\n",
      "./python_model_objects/uplift/ordEnc_macd-cross-0.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "macd-oversold\n",
      "\n",
      "saved ordinal encoder for macd-oversold at\n",
      "./python_model_objects/uplift/ordEnc_macd-oversold.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "mid-term-sideway\n",
      "\n",
      "saved ordinal encoder for mid-term-sideway at\n",
      "./python_model_objects/uplift/ordEnc_mid-term-sideway.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "near-support\n",
      "\n",
      "saved ordinal encoder for near-support at\n",
      "./python_model_objects/uplift/ordEnc_near-support.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "on-balance-volume-obv\n",
      "\n",
      "saved ordinal encoder for on-balance-volume-obv at\n",
      "./python_model_objects/uplift/ordEnc_on-balance-volume-obv.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "oversold-bullish-engulfing\n",
      "\n",
      "saved ordinal encoder for oversold-bullish-engulfing at\n",
      "./python_model_objects/uplift/ordEnc_oversold-bullish-engulfing.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "parabolic-sar\n",
      "\n",
      "saved ordinal encoder for parabolic-sar at\n",
      "./python_model_objects/uplift/ordEnc_parabolic-sar.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "risk-reward-ratio-chandelier-exit\n",
      "\n",
      "saved ordinal encoder for risk-reward-ratio-chandelier-exit at\n",
      "./python_model_objects/uplift/ordEnc_risk-reward-ratio-chandelier-exit.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "rsi-above-50\n",
      "\n",
      "saved ordinal encoder for rsi-above-50 at\n",
      "./python_model_objects/uplift/ordEnc_rsi-above-50.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "rsi-oversold\n",
      "\n",
      "saved ordinal encoder for rsi-oversold at\n",
      "./python_model_objects/uplift/ordEnc_rsi-oversold.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "sharpe-ratio\n",
      "\n",
      "saved ordinal encoder for sharpe-ratio at\n",
      "./python_model_objects/uplift/ordEnc_sharpe-ratio.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "short-term-oversold\n",
      "\n",
      "saved ordinal encoder for short-term-oversold at\n",
      "./python_model_objects/uplift/ordEnc_short-term-oversold.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "short-term-sideway\n",
      "\n",
      "saved ordinal encoder for short-term-sideway at\n",
      "./python_model_objects/uplift/ordEnc_short-term-sideway.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "silence-is-golden\n",
      "\n",
      "saved ordinal encoder for silence-is-golden at\n",
      "./python_model_objects/uplift/ordEnc_silence-is-golden.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "simple-uptrend\n",
      "\n",
      "saved ordinal encoder for simple-uptrend at\n",
      "./python_model_objects/uplift/ordEnc_simple-uptrend.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "sma20-cross-sma40\n",
      "\n",
      "saved ordinal encoder for sma20-cross-sma40 at\n",
      "./python_model_objects/uplift/ordEnc_sma20-cross-sma40.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "smart-money\n",
      "\n",
      "saved ordinal encoder for smart-money at\n",
      "./python_model_objects/uplift/ordEnc_smart-money.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "solid-ma-uptrend\n",
      "\n",
      "saved ordinal encoder for solid-ma-uptrend at\n",
      "./python_model_objects/uplift/ordEnc_solid-ma-uptrend.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "stochastic-overbought\n",
      "\n",
      "saved ordinal encoder for stochastic-overbought at\n",
      "./python_model_objects/uplift/ordEnc_stochastic-overbought.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "stochastic-oversold\n",
      "\n",
      "saved ordinal encoder for stochastic-oversold at\n",
      "./python_model_objects/uplift/ordEnc_stochastic-oversold.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "tenkan-kijun-cross\n",
      "\n",
      "saved ordinal encoder for tenkan-kijun-cross at\n",
      "./python_model_objects/uplift/ordEnc_tenkan-kijun-cross.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "tplus\n",
      "\n",
      "saved ordinal encoder for tplus at\n",
      "./python_model_objects/uplift/ordEnc_tplus.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "tplus-volume\n",
      "\n",
      "saved ordinal encoder for tplus-volume at\n",
      "./python_model_objects/uplift/ordEnc_tplus-volume.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "volume-engulfing\n",
      "\n",
      "saved ordinal encoder for volume-engulfing at\n",
      "./python_model_objects/uplift/ordEnc_volume-engulfing.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "volume-increase-30-percent\n",
      "\n",
      "saved ordinal encoder for volume-increase-30-percent at\n",
      "./python_model_objects/uplift/ordEnc_volume-increase-30-percent.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "vwma-as-support\n",
      "\n",
      "saved ordinal encoder for vwma-as-support at\n",
      "./python_model_objects/uplift/ordEnc_vwma-as-support.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "day_name\n",
      "\n",
      "saved ordinal encoder for day_name at\n",
      "./python_model_objects/uplift/ordEnc_day_name.pkl\n",
      "\n",
      "encoding categorical column:\n",
      "month\n",
      "\n",
      "saved ordinal encoder for month at\n",
      "./python_model_objects/uplift/ordEnc_month.pkl\n",
      "\n",
      "Fitting 5 folds for each of 2500 candidates, totalling 12500 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-464-562a6df691fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;31m# Search for best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams_for_fitting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mnew_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1754\u001b[0m             ParameterSampler(\n\u001b[0;32m   1755\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    820\u001b[0m                     )\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    823\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1942\u001b[0m         \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1944\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1586\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1587\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Amirul\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1697\u001b[0m                 (self._jobs[0].get_status(\n\u001b[0;32m   1698\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scenario_list = ['d3_p3', 'd3_p5', 'd3_p7', 'd5_p3', 'd5_p5', 'd5_p7', 'd7_p3', 'd7_p5', 'd3_p7']\n",
    "for scenario in scenario_list:\n",
    "    # Script variables\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    script_start = datetime.now()       # Script start\n",
    "    _seed = 999                         # random state seed\n",
    "    save_transformers = True            # Save the lgb transformers\n",
    "    verbose_script = True               # Verbosity of script\n",
    "\n",
    "    mdl_nm = 'uplift'\n",
    "\n",
    "    target = f'target_{scenario}'\n",
    "\n",
    "    # metric_to_use = 'average_precision'\n",
    "    #metric_to_use = 'auc'\n",
    "    metric_to_use = 'precision'\n",
    "    #metric_to_use = 'auc'\n",
    "\n",
    "    #scoring_to_use = 'roc_auc'\n",
    "    # scoring_to_use = 'average_precision'\n",
    "    scoring_to_use = 'precision'\n",
    "    #scoring_to_use = 'log_loss'\n",
    "\n",
    "    #exp_set = 'set2_' + metric_to_use\n",
    "    exp_set = 'tuneAUC_'\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Set Features to use in model\n",
    "    #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "    no_cat_features = False\n",
    "    \n",
    "\n",
    "    # Save dataset dictionary\n",
    "    dict_path = f'./{mdl_nm}/'\n",
    "    f_name = dict_path + f'dataset_schema_dict.pkl'\n",
    "\n",
    "    with open(f_name, 'wb') as handle:\n",
    "        pickle.dump(\n",
    "            dataset_schema, \n",
    "            handle, \n",
    "            protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Set Model Parameters\n",
    "    #######################################################################\n",
    "    \n",
    "        \n",
    "    # set parameters\n",
    "    init_param_dict = {                      # !!! TUNE THESE\n",
    "        'seed': _seed,\n",
    "        'num_threads': 0,\n",
    "        'verbosity': 0,\n",
    "        #\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'num_iterations': 2000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'tree_learner': 'data',\n",
    "        'subsample_for_bin':300000, # lower=performance, high=accuracy\n",
    "        #\n",
    "        'metric': metric_to_use,\n",
    "    }\n",
    "\n",
    "    init_param_dict['categorical_feature'] = 'name:'\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Load Data\n",
    "    #######################################################################\n",
    "\n",
    "    df = train\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # \n",
    "    #######################################################################\n",
    "\n",
    "    #Set index ....\n",
    "    # df.set_index(['Name','date'], inplace = True)\n",
    "\n",
    "    if verbose_script:\n",
    "        print('loaded data')\n",
    "        print(datetime.now() - script_start)\n",
    "        print('')\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Remove bad columns\n",
    "    #######################################################################\n",
    "\n",
    "    # Remove other cols\n",
    "    omit_cols = [\n",
    "        c for c in df.columns\n",
    "        if (\n",
    "            'lead' in c.lower() \n",
    "            or c.startswith('target_')\n",
    "                \n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Ensure we're not going to remove the target\n",
    "    if target in omit_cols:\n",
    "        omit_cols.remove(target)\n",
    "\n",
    "    df.drop(columns = omit_cols, inplace = True)\n",
    "\n",
    "    # Remove 0-variance columns:\n",
    "    df = df.loc[:,df.apply(pd.Series.nunique) > 1].copy(deep=True)\n",
    "\n",
    "    #######################################################################\n",
    "    # Ordinal (integer) Encode categorical variables\n",
    "    #######################################################################\n",
    "\n",
    "    for col in df.drop(columns=[target]).columns:\n",
    "\n",
    "        if df[col].dtype == object or df[col].dtype.name == 'category':\n",
    "            if verbose_script:\n",
    "                print('encoding categorical column:')\n",
    "                print(col)\n",
    "                print('')\n",
    "\n",
    "            # # Fill missing values\n",
    "            # df[col].fillna('__unk__', inplace = True)\n",
    "\n",
    "            # Instantiate sklearn's encoder class\n",
    "            cnt_unique_values = df[col].nunique()\n",
    "\n",
    "            if cnt_unique_values <= (255 - 3):\n",
    "                dtype_to_use = np.uint8\n",
    "            elif cnt_unique_values <= (65535 - 3) :\n",
    "                dtype_to_use = np.int16\n",
    "            elif cnt_unique_values <= (4294967295 - 3):\n",
    "                dtype_to_use = np.int32\n",
    "            else:\n",
    "                dtype_to_use = np.uint64\n",
    "\n",
    "            encoder = OrdinalEncoder(\n",
    "                categories = 'auto',\n",
    "                dtype = dtype_to_use,\n",
    "                handle_unknown = 'use_encoded_value',\n",
    "                unknown_value = cnt_unique_values + 2,\n",
    "            )\n",
    "\n",
    "            # Fit the encoder\n",
    "            encoder.fit(df[col].to_numpy().reshape(-1,1))\n",
    "\n",
    "            # Transform the df object values\n",
    "            new_values = encoder.transform(df[col].to_numpy().reshape(-1,1))\n",
    "            \n",
    "            df[col] = pd.Categorical(new_values.ravel(), ordered = False)\n",
    "\n",
    "            # Save encoder object\n",
    "            encoder_path = f'./python_model_objects/{mdl_nm}/'\n",
    "            f_name = encoder_path + f'ordEnc_{col}.pkl'\n",
    "\n",
    "            with open(f_name, 'wb') as handle:\n",
    "                pickle.dump(\n",
    "                    encoder, \n",
    "                    handle, \n",
    "                    protocol=pickle.HIGHEST_PROTOCOL\n",
    "                )\n",
    "                \n",
    "            init_param_dict['categorical_feature'] += col + ','\n",
    "            \n",
    "            if verbose_script:\n",
    "                print(f'saved ordinal encoder for {col} at')\n",
    "                print(f_name)\n",
    "                print('')\n",
    "\n",
    "\n",
    "    # remove the last comma\n",
    "    init_param_dict['categorical_feature'] = init_param_dict['categorical_feature'][:-1]\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Train / Test Splits\n",
    "    #######################################################################\n",
    "\n",
    "    df = df[df[target].notnull()].copy(deep=True)\n",
    "\n",
    "    X_ = df.drop(columns=target).copy(deep=True)\n",
    "    y_ = df[target].copy(deep=True)\n",
    "\n",
    "    # Shuffle data\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X_ , y_, test_size = 0.3, random_state = _seed, stratify = y_)\n",
    "        \n",
    "    # Add column names...\n",
    "    init_param_dict['feature_name'] = list(X_.columns)\n",
    "\n",
    "    #######################################################################\n",
    "    # LGB Model Object\n",
    "    #######################################################################\n",
    "\n",
    "    # Instantiate obj\n",
    "    lgb_classifier = lgb.LGBMClassifier(**init_param_dict)\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # SKLearn Cross Val Object\n",
    "    #######################################################################\n",
    "\n",
    "    # Splitting strategy for Cross-Validation\n",
    "    sss_cv = StratifiedShuffleSplit(\n",
    "        n_splits = 5,\n",
    "        test_size = 0.3, \n",
    "        train_size = 0.7, \n",
    "        random_state = _seed\n",
    "    )\n",
    "\n",
    "    #######################################################################\n",
    "    # SKLearn Search Object\n",
    "    #######################################################################\n",
    "    # Default list of values to search in naive cases\n",
    "    values_to_check = sorted(set([k ** n for n in range(-8,3) for k in range(1,10+1)]))\n",
    "\n",
    "    # Params to search\n",
    "    params_to_optimize = {\n",
    "        #'max_depth': [3,4,5,6,7,8,9,10,20],\n",
    "        \n",
    "        'max_depth': [5,10,15,20,30,50],    \n",
    "        # 'num_leaves': [10, 25, 50, 75, 100, 150, 200, 500],\n",
    "        'num_leaves': [5, 10, 25, 50, 75, 100, 150],\n",
    "        # 'learning_rate': [value for value in values_to_check if value < 1.0],\n",
    "        'min_data_in_leaf': [20, 50,100,250],\n",
    "        # 'min_child_weight': [0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "        # 'min_gain_to_split': [0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1], \n",
    "        # 'bagging_fraction': [0.05, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
    "        'bagging_freq': [2, 5, 10, 25, 50],\n",
    "        'feature_fraction': [0.05, 0.25, 0.5, 0.75, 0.9, 1],\n",
    "        # 'lambda_l1': values_to_check,\n",
    "        # 'lambda_l2': values_to_check,\n",
    "        'scale_pos_weight':[0.1, 0.5, 1, 2, 5, 10, 25]\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Cross validation with randomized search\n",
    "    clf = RandomizedSearchCV(\n",
    "        estimator = lgb_classifier, \n",
    "        param_distributions = params_to_optimize,   ##TUNE\n",
    "        n_iter = 2500,                              ##TUNE\n",
    "        random_state = _seed,\n",
    "        n_jobs = -1, \n",
    "        refit = True, \n",
    "        cv = sss_cv,\n",
    "        scoring = scoring_to_use,\n",
    "        verbose = 1, \n",
    "        return_train_score = False,\n",
    "    )\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Fit model\n",
    "    #######################################################################\n",
    "\n",
    "    # Params for fit function\n",
    "    params_for_fitting = {\n",
    "        'X': X_train, \n",
    "        'y': y_train, \n",
    "        'eval_set': [(X_eval, y_eval)],\n",
    "        'eval_metric': 'auc',\n",
    "        'feature_name': init_param_dict['feature_name'],\n",
    "        'categorical_feature': init_param_dict['categorical_feature'][5:].split(','),\n",
    "    }\n",
    "\n",
    "\n",
    "    cv = True\n",
    "\n",
    "    if cv:\n",
    "    \n",
    "        # Search for best parameters\n",
    "        search = clf.fit(**params_for_fitting)  \n",
    "    \n",
    "        new_params = lgb_classifier.get_params()\n",
    "        new_params.update(search.best_params_)\n",
    "        \n",
    "        if no_cat_features:\n",
    "            del new_params['categorical_feature']\n",
    "            del params_for_fitting['categorical_feature']\n",
    "            \n",
    "        lgb_classifier = lgb.LGBMClassifier(**new_params)\n",
    "\n",
    "        lgbmCV = lgb_classifier.fit(**params_for_fitting)\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        learning_params = {\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 200,\n",
    "        'learning_rate': 0.01,\n",
    "        'min_data_in_leaf': 25,\n",
    "        'min_child_weight': 0.0001,\n",
    "        'min_gain_to_split': 0.0001, \n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': 1.0,\n",
    "        'lambda_l1': 0,\n",
    "        'lambda_l2': 0,\n",
    "        'early_stopping_rounds':100,\n",
    "        }\n",
    "        \n",
    "        new_params = lgb_classifier.get_params()\n",
    "        new_params.update(learning_params)\n",
    "        \n",
    "        if no_cat_features:\n",
    "            del new_params['categorical_feature']\n",
    "            del params_for_fitting['categorical_feature']\n",
    "        \n",
    "        lgb_classifier = lgb.LGBMClassifier(**new_params)\n",
    "\n",
    "        lgbmCV = lgb_classifier.fit(**params_for_fitting)\n",
    "        \n",
    "\n",
    "        \n",
    "    #######################################################################\n",
    "    # Save Model\n",
    "    #######################################################################\n",
    "\n",
    "    model_path = f'./python_model_objects/{mdl_nm}/'\n",
    "    f_name = model_path + f'{scenario}_lgbmcv_model.pkl'\n",
    "\n",
    "    with open(f_name, 'wb') as handle:\n",
    "        pickle.dump(\n",
    "            lgbmCV, \n",
    "            handle, \n",
    "            protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "    print (\"Model saved!!!!\")\n",
    "\n",
    "    #######################################################################\n",
    "    # \n",
    "    #       Amirul : 20230531\n",
    "    #\n",
    "    #\n",
    "    #######################################################################\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Generate predictions\n",
    "    #######################################################################\n",
    "    feature_names = lgbmCV.feature_name\n",
    "\n",
    "    df_eval = X_eval.join(y_eval)\n",
    "    df_eval['y_pred'] = lgbmCV.predict_proba(df_eval[feature_names])[:,1]\n",
    "    df_eval['y_pred_class'] = lgbmCV.predict(df_eval[feature_names])\n",
    "    #df['y_pred_class'] = (model_obj.predict_proba(df[feature_names])[:,1] >= 0.5).astype(bool)\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    # Calculate Metric\n",
    "    #######################################################################\n",
    "    auc_score = roc_auc_score(y_true = df_eval[target].astype(int), y_score = df_eval['y_pred'])\n",
    "    precision = precision_score(df_eval[target].astype(int), df_eval['y_pred_class'])\n",
    "    recall = recall_score(df_eval[target].astype(int), df_eval['y_pred_class'])\n",
    "\n",
    "    print ('AUC Score:' , auc_score)\n",
    "    print ('Precision:' , precision)\n",
    "    print ('Recall:' , recall )\n",
    "\n",
    "\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df_eval[target], df_eval['y_pred_class'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    #confusion_matrix.to_csv(exp_set + f'train_confusion_matrix.csv')\n",
    "    with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", engine=\"openpyxl\") as writer: \n",
    "        confusion_matrix.to_excel(writer, sheet_name=\"cf\", index=True)\n",
    "\n",
    "\n",
    "    metric_result = pd.DataFrame({'AUC' : [auc_score], 'Precision' : [precision], 'Recall': [recall]})\n",
    "    metric_result.to_csv(exp_set + f'train_metric.csv')\n",
    "\n",
    "    #######################################################################\n",
    "    # Create table\n",
    "    #######################################################################\n",
    "    lenx = len(df_eval)\n",
    "\n",
    "    for qtile,n_ in [('decile',10), ('percentile',lenx)]:\n",
    "        # Add the qtile label to rows\n",
    "        df_eval[target] = df_eval[target].astype('int')\n",
    "        df_eval[qtile] = pd.qcut(df_eval['y_pred'].rank(method='first', ascending=False), n_, labels=False)\n",
    "\n",
    "        # Aggregate rows to qtile bin\n",
    "        agg_df = df_eval.groupby(qtile).agg(\n",
    "            {qtile:len , target:np.nansum, 'y_pred':[np.nanmin, np.nanmax]})\n",
    "\n",
    "        agg_df.columns = ['total_cnt','positives','pred_min', 'pred_max']\n",
    "\n",
    "        agg_df['positive_rate'] = agg_df['positives'] / agg_df['total_cnt']\n",
    "\n",
    "        avg_capture = np.sum(agg_df.positives) / np.sum(agg_df.total_cnt)\n",
    "\n",
    "        agg_df['decile_lift'] = agg_df['positive_rate'] / avg_capture\n",
    "\n",
    "        agg_df.sort_index(ascending = True, inplace = True)\n",
    "\n",
    "        agg_df['cumsum_total_cnt'] = agg_df.total_cnt.cumsum()\n",
    "\n",
    "        agg_df['cumsum_positives'] = agg_df.positives.cumsum()\n",
    "\n",
    "\n",
    "        if qtile == 'decile':\n",
    "            range_size = 10\n",
    "        else:\n",
    "            range_size = lenx\n",
    "\n",
    "\n",
    "        agg_df['naive_rt'] = [(i+1)/range_size for i in range(range_size)]\n",
    "\n",
    "        agg_df['naive_cumsum_positives'] = agg_df.naive_rt * np.sum(agg_df.positives)\n",
    "\n",
    "        agg_df['total_gain'] = agg_df.cumsum_positives / agg_df.naive_cumsum_positives\n",
    "\n",
    "        agg_df['precision_test'] = agg_df.cumsum_positives / agg_df.cumsum_total_cnt \n",
    "\n",
    "        #qtile_dfs[qtile] = agg_df.copy(deep=True)\n",
    "        \n",
    "        with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer: \n",
    "            agg_df.to_excel(writer, sheet_name=f'train_{qtile}', index=True)\n",
    "            # agg_df.to_csv(f'{exp_set}_train_lift_{qtile}.csv')\n",
    "\n",
    "        \n",
    "    #######################################################################\n",
    "    # Print Feature Importance\n",
    "    #######################################################################\n",
    "\n",
    "    feature_impt = pd.DataFrame({'Value':lgbmCV.feature_importances_,'Feature':lgbmCV.feature_name}).sort_values(by = 'Value', ascending = False)\n",
    "    with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer: \n",
    "        feature_impt.to_excel(writer, sheet_name=(f'fi_train_{qtile}'), index=True)\n",
    "        #feature_impt.to_csv(exp_set + f'feature_importance_{exp_set}_train_lift_{qtile}.csv')\n",
    "\n",
    "\n",
    "\n",
    "    print (\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
