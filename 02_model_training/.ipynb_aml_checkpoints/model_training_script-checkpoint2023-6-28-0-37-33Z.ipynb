{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install imblearn\n",
        "!pip3 install deployed\n",
        "!pip3 install xgboost\n",
        "!pip3 install plotly\n",
        "!pip3 install lightgbm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: imblearn in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imblearn) (0.11.0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.0)\nRequirement already satisfied: scipy>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.2.0)\nRequirement already satisfied: numpy>=1.17.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.21.6)\n\u001b[31mERROR: Could not find a version that satisfies the requirement deployed (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for deployed\u001b[0m\nRequirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.3.3)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from xgboost) (1.21.6)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from xgboost) (1.5.3)\nRequirement already satisfied: plotly in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (5.15.0)\nRequirement already satisfied: tenacity>=6.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from plotly) (8.2.2)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from plotly) (23.0)\nRequirement already satisfied: lightgbm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (3.2.1)\nRequirement already satisfied: scikit-learn!=0.22.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from lightgbm) (1.3.0)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from lightgbm) (1.5.3)\nRequirement already satisfied: wheel in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from lightgbm) (0.37.1)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from lightgbm) (1.21.6)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1690502723445
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# Import Python Modules\n",
        "####################################\n",
        "# General Purpose Modules\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "# Data Processing Modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ML Modules\n",
        "conda install -c conda-forge lightgbm\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score,precision_score, recall_score \n",
        "\n",
        "\n",
        "# Custom modules\n",
        "from dataset_schema_dict import dataset_schema\n",
        "\n",
        "seed = 99\n",
        "_seed = 99"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lightgbm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ML Modules\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1690502724337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('research_data_7days_above_20_2023.csv', index_col=False)\n",
        "data = data.dropna()\n",
        "\n",
        "data['target_d3_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.03*data['t0']), 1, 0)))))\n",
        "\n",
        "data['target_d5_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.03*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.03*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.03*data['t0']), 1, 0)))))))))\n",
        "\n",
        "data['target_d7_p3'] = np.where(data['t1']> (1.03*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.03*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.03*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.03*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.03*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t6']> (1.03*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t7']> (1.03*data['t0']), 1, 0)))))))))))))\n",
        "\n",
        "data['target_d3_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.05*data['t0']), 1, 0)))))\n",
        "\n",
        "data['target_d5_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.05*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.05*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.05*data['t0']), 1, 0)))))))))\n",
        "\n",
        "\n",
        "data['target_d7_p5'] = np.where(data['t1']> (1.05*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.05*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.05*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.05*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.05*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t6']> (1.05*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t7']> (1.05*data['t0']), 1, 0)))))))))))))\n",
        "\n",
        "\n",
        "data['target_d3_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.07*data['t0']), 1, 0)))))\n",
        "\n",
        "data['target_d5_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.07*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.07*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.07*data['t0']), 1, 0)))))))))\n",
        "\n",
        "\n",
        "data['target_d7_p7'] = np.where(data['t1']> (1.07*data['t0']), 1, np.where(data['t1']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t2']> (1.07*data['t0']), 1, np.where(data['t2']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t3']> (1.07*data['t0']), 1 , np.where(data['t3']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t4']> (1.07*data['t0']), 1 , np.where(data['t4']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t5']> (1.07*data['t0']), 1 , np.where(data['t5']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t6']> (1.07*data['t0']), 1 , np.where(data['t6']<(0.97*data['t0']), 0, \n",
        "               np.where(data['t7']> (1.07*data['t0']), 1, 0)))))))))))))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724573
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['day_name'] = pd.to_datetime(data['date'], format='%d/%m/%Y').dt.day_name()\n",
        "data['month'] = pd.to_datetime(data['date'], format='%d/%m/%Y').dt.strftime('%m')\n",
        "data['month'].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724624
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integer_columns = data.select_dtypes(include=['int64','int32']).columns\n",
        "float_columns = data.select_dtypes(include=['float']).columns\n",
        "object_columns = data.select_dtypes(include=['object']).columns\n",
        "data[integer_columns] = data[integer_columns].astype('category')\n",
        "\n",
        "data.dtypes.value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_list = [col for col in data if col.startswith('target')]\n",
        "\n",
        "for col in target_list:\n",
        "    print (col)\n",
        "    print(data[col].value_counts(normalize=True))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724655
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario = 'd7_p5'\n",
        "data[['t0', 't1','t2','t3','t4','t5','t6','t7']][data[f'target_{scenario}'] == 1]\n",
        "#data[['date','t0', 't1','t2','t3','t4','t5','t6','t7']][data['Name'] == 'GRANFLO']\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724671
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Define Train OOT (random split)\n",
        "\n",
        "df1 = data.drop(columns=['t1', 't2', 't3', 't4', 't5', 't6', 't7'])\n",
        "df1.set_index(['Name', 'date'], inplace=True)\n",
        "\n",
        "def train_validate_test_split(df, train_percent=.8, seed=seed):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df)\n",
        "    train_end = int(train_percent * m)\n",
        "    train = df.loc[perm[:train_end]]\n",
        "    oot = df.loc[perm[train_end:]]\n",
        "    return train, oot\n",
        "\n",
        "train, oot = train_validate_test_split(df1)\n",
        "\n",
        "print (data.shape)\n",
        "print (f'Split OK? : {len(train) + len(oot)== len(data)}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724685
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del data\n",
        "del df1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724702
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scenario_list = ['d3_p3', 'd3_p5', 'd3_p7', 'd5_p3', 'd5_p5', 'd5_p7', 'd7_p3', 'd7_p5', 'd3_p7']\n",
        "for scenario in scenario_list:\n",
        "    # Script variables\n",
        "    pd.options.mode.chained_assignment = None\n",
        "\n",
        "    script_start = datetime.now()       # Script start\n",
        "    _seed = 999                         # random state seed\n",
        "    save_transformers = True            # Save the lgb transformers\n",
        "    verbose_script = True               # Verbosity of script\n",
        "\n",
        "    mdl_nm = 'uplift'\n",
        "\n",
        "    target = f'target_{scenario}'\n",
        "\n",
        "    # metric_to_use = 'average_precision'\n",
        "    #metric_to_use = 'auc'\n",
        "    metric_to_use = 'precision'\n",
        "    #metric_to_use = 'auc'\n",
        "\n",
        "    #scoring_to_use = 'roc_auc'\n",
        "    # scoring_to_use = 'average_precision'\n",
        "    scoring_to_use = 'precision'\n",
        "    #scoring_to_use = 'log_loss'\n",
        "\n",
        "    #exp_set = 'set2_' + metric_to_use\n",
        "    exp_set = 'tuneAUC_'\n",
        "\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Set Features to use in model\n",
        "    #######################################################################\n",
        "\n",
        "\n",
        "\n",
        "    no_cat_features = False\n",
        "    \n",
        "\n",
        "    # Save dataset dictionary\n",
        "    dict_path = f'./{mdl_nm}/'\n",
        "    f_name = dict_path + f'dataset_schema_dict.pkl'\n",
        "\n",
        "    with open(f_name, 'wb') as handle:\n",
        "        pickle.dump(\n",
        "            dataset_schema, \n",
        "            handle, \n",
        "            protocol=pickle.HIGHEST_PROTOCOL\n",
        "        )\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Set Model Parameters\n",
        "    #######################################################################\n",
        "    \n",
        "        \n",
        "    # set parameters\n",
        "    init_param_dict = {                      # !!! TUNE THESE\n",
        "        'seed': _seed,\n",
        "        'num_threads': 0,\n",
        "        'verbosity': 0,\n",
        "        #\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'num_iterations': 2000,\n",
        "        'early_stopping_rounds': 50,\n",
        "        'tree_learner': 'data',\n",
        "        'subsample_for_bin':300000, # lower=performance, high=accuracy\n",
        "        #\n",
        "        'metric': metric_to_use,\n",
        "    }\n",
        "\n",
        "    init_param_dict['categorical_feature'] = 'name:'\n",
        "\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Load Data\n",
        "    #######################################################################\n",
        "\n",
        "    df = train\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # \n",
        "    #######################################################################\n",
        "\n",
        "    #Set index ....\n",
        "    # df.set_index(['Name','date'], inplace = True)\n",
        "\n",
        "    if verbose_script:\n",
        "        print('loaded data')\n",
        "        print(datetime.now() - script_start)\n",
        "        print('')\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Remove bad columns\n",
        "    #######################################################################\n",
        "\n",
        "    # Remove other cols\n",
        "    omit_cols = [\n",
        "        c for c in df.columns\n",
        "        if (\n",
        "            'lead' in c.lower() \n",
        "            or c.startswith('target_')\n",
        "                \n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Ensure we're not going to remove the target\n",
        "    if target in omit_cols:\n",
        "        omit_cols.remove(target)\n",
        "\n",
        "    df.drop(columns = omit_cols, inplace = True)\n",
        "\n",
        "    # Remove 0-variance columns:\n",
        "    df = df.loc[:,df.apply(pd.Series.nunique) > 1].copy(deep=True)\n",
        "\n",
        "    #######################################################################\n",
        "    # Ordinal (integer) Encode categorical variables\n",
        "    #######################################################################\n",
        "\n",
        "    for col in df.drop(columns=[target]).columns:\n",
        "\n",
        "        if df[col].dtype == object or df[col].dtype.name == 'category':\n",
        "            if verbose_script:\n",
        "                print('encoding categorical column:')\n",
        "                print(col)\n",
        "                print('')\n",
        "\n",
        "            # # Fill missing values\n",
        "            # df[col].fillna('__unk__', inplace = True)\n",
        "\n",
        "            # Instantiate sklearn's encoder class\n",
        "            cnt_unique_values = df[col].nunique()\n",
        "\n",
        "            if cnt_unique_values <= (255 - 3):\n",
        "                dtype_to_use = np.uint8\n",
        "            elif cnt_unique_values <= (65535 - 3) :\n",
        "                dtype_to_use = np.int16\n",
        "            elif cnt_unique_values <= (4294967295 - 3):\n",
        "                dtype_to_use = np.int32\n",
        "            else:\n",
        "                dtype_to_use = np.uint64\n",
        "\n",
        "            encoder = OrdinalEncoder(\n",
        "                categories = 'auto',\n",
        "                dtype = dtype_to_use,\n",
        "                handle_unknown = 'use_encoded_value',\n",
        "                unknown_value = cnt_unique_values + 2,\n",
        "            )\n",
        "\n",
        "            # Fit the encoder\n",
        "            encoder.fit(df[col].to_numpy().reshape(-1,1))\n",
        "\n",
        "            # Transform the df object values\n",
        "            new_values = encoder.transform(df[col].to_numpy().reshape(-1,1))\n",
        "            \n",
        "            df[col] = pd.Categorical(new_values.ravel(), ordered = False)\n",
        "\n",
        "            # Save encoder object\n",
        "            encoder_path = f'./python_model_objects/{mdl_nm}/'\n",
        "            f_name = encoder_path + f'ordEnc_{col}.pkl'\n",
        "\n",
        "            with open(f_name, 'wb') as handle:\n",
        "                pickle.dump(\n",
        "                    encoder, \n",
        "                    handle, \n",
        "                    protocol=pickle.HIGHEST_PROTOCOL\n",
        "                )\n",
        "                \n",
        "            init_param_dict['categorical_feature'] += col + ','\n",
        "            \n",
        "            if verbose_script:\n",
        "                print(f'saved ordinal encoder for {col} at')\n",
        "                print(f_name)\n",
        "                print('')\n",
        "\n",
        "\n",
        "    # remove the last comma\n",
        "    init_param_dict['categorical_feature'] = init_param_dict['categorical_feature'][:-1]\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Train / Test Splits\n",
        "    #######################################################################\n",
        "\n",
        "    df = df[df[target].notnull()].copy(deep=True)\n",
        "\n",
        "    X_ = df.drop(columns=target).copy(deep=True)\n",
        "    y_ = df[target].copy(deep=True)\n",
        "\n",
        "    # Shuffle data\n",
        "    X_train, X_eval, y_train, y_eval = train_test_split(X_ , y_, test_size = 0.3, random_state = _seed, stratify = y_)\n",
        "        \n",
        "    # Add column names...\n",
        "    init_param_dict['feature_name'] = list(X_.columns)\n",
        "\n",
        "    #######################################################################\n",
        "    # LGB Model Object\n",
        "    #######################################################################\n",
        "\n",
        "    # Instantiate obj\n",
        "    lgb_classifier = lgb.LGBMClassifier(**init_param_dict)\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # SKLearn Cross Val Object\n",
        "    #######################################################################\n",
        "\n",
        "    # Splitting strategy for Cross-Validation\n",
        "    sss_cv = StratifiedShuffleSplit(\n",
        "        n_splits = 5,\n",
        "        test_size = 0.3, \n",
        "        train_size = 0.7, \n",
        "        random_state = _seed\n",
        "    )\n",
        "\n",
        "    #######################################################################\n",
        "    # SKLearn Search Object\n",
        "    #######################################################################\n",
        "    # Default list of values to search in naive cases\n",
        "    values_to_check = sorted(set([k ** n for n in range(-8,3) for k in range(1,10+1)]))\n",
        "\n",
        "    # Params to search\n",
        "    params_to_optimize = {\n",
        "        #'max_depth': [3,4,5,6,7,8,9,10,20],\n",
        "        \n",
        "        'max_depth': [5,10,15,20,30,50],    \n",
        "        # 'num_leaves': [10, 25, 50, 75, 100, 150, 200, 500],\n",
        "        'num_leaves': [5, 10, 25, 50, 75, 100, 150],\n",
        "        # 'learning_rate': [value for value in values_to_check if value < 1.0],\n",
        "        'min_data_in_leaf': [20, 50,100,250],\n",
        "        # 'min_child_weight': [0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1],\n",
        "        # 'min_gain_to_split': [0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1], \n",
        "        # 'bagging_fraction': [0.05, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
        "        'bagging_freq': [2, 5, 10, 25, 50],\n",
        "        'feature_fraction': [0.05, 0.25, 0.5, 0.75, 0.9, 1],\n",
        "        # 'lambda_l1': values_to_check,\n",
        "        # 'lambda_l2': values_to_check,\n",
        "        'scale_pos_weight':[0.1, 0.5, 1, 2, 5, 10, 25]\n",
        "        \n",
        "    }\n",
        "\n",
        "    # Cross validation with randomized search\n",
        "    clf = RandomizedSearchCV(\n",
        "        estimator = lgb_classifier, \n",
        "        param_distributions = params_to_optimize,   ##TUNE\n",
        "        n_iter = 2500,                              ##TUNE\n",
        "        random_state = _seed,\n",
        "        n_jobs = -1, \n",
        "        refit = True, \n",
        "        cv = sss_cv,\n",
        "        scoring = scoring_to_use,\n",
        "        verbose = 1, \n",
        "        return_train_score = False,\n",
        "    )\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Fit model\n",
        "    #######################################################################\n",
        "\n",
        "    # Params for fit function\n",
        "    params_for_fitting = {\n",
        "        'X': X_train, \n",
        "        'y': y_train, \n",
        "        'eval_set': [(X_eval, y_eval)],\n",
        "        'eval_metric': 'auc',\n",
        "        'feature_name': init_param_dict['feature_name'],\n",
        "        'categorical_feature': init_param_dict['categorical_feature'][5:].split(','),\n",
        "    }\n",
        "\n",
        "\n",
        "    cv = True\n",
        "\n",
        "    if cv:\n",
        "    \n",
        "        # Search for best parameters\n",
        "        search = clf.fit(**params_for_fitting)  \n",
        "    \n",
        "        new_params = lgb_classifier.get_params()\n",
        "        new_params.update(search.best_params_)\n",
        "        \n",
        "        if no_cat_features:\n",
        "            del new_params['categorical_feature']\n",
        "            del params_for_fitting['categorical_feature']\n",
        "            \n",
        "        lgb_classifier = lgb.LGBMClassifier(**new_params)\n",
        "\n",
        "        lgbmCV = lgb_classifier.fit(**params_for_fitting)\n",
        "    \n",
        "        \n",
        "    else:\n",
        "        learning_params = {\n",
        "        'max_depth': 5,\n",
        "        'num_leaves': 200,\n",
        "        'learning_rate': 0.01,\n",
        "        'min_data_in_leaf': 25,\n",
        "        'min_child_weight': 0.0001,\n",
        "        'min_gain_to_split': 0.0001, \n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'feature_fraction': 1.0,\n",
        "        'lambda_l1': 0,\n",
        "        'lambda_l2': 0,\n",
        "        'early_stopping_rounds':100,\n",
        "        }\n",
        "        \n",
        "        new_params = lgb_classifier.get_params()\n",
        "        new_params.update(learning_params)\n",
        "        \n",
        "        if no_cat_features:\n",
        "            del new_params['categorical_feature']\n",
        "            del params_for_fitting['categorical_feature']\n",
        "        \n",
        "        lgb_classifier = lgb.LGBMClassifier(**new_params)\n",
        "\n",
        "        lgbmCV = lgb_classifier.fit(**params_for_fitting)\n",
        "        \n",
        "\n",
        "        \n",
        "    #######################################################################\n",
        "    # Save Model\n",
        "    #######################################################################\n",
        "\n",
        "    model_path = f'./python_model_objects/{mdl_nm}/'\n",
        "    f_name = model_path + f'{scenario}_lgbmcv_model.pkl'\n",
        "\n",
        "    with open(f_name, 'wb') as handle:\n",
        "        pickle.dump(\n",
        "            lgbmCV, \n",
        "            handle, \n",
        "            protocol=pickle.HIGHEST_PROTOCOL\n",
        "        )\n",
        "\n",
        "    print (\"Model saved!!!!\")\n",
        "\n",
        "    #######################################################################\n",
        "    # \n",
        "    #       Amirul : 20230531\n",
        "    #\n",
        "    #\n",
        "    #######################################################################\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Generate predictions\n",
        "    #######################################################################\n",
        "    feature_names = lgbmCV.feature_name\n",
        "\n",
        "    df_eval = X_eval.join(y_eval)\n",
        "    df_eval['y_pred'] = lgbmCV.predict_proba(df_eval[feature_names])[:,1]\n",
        "    df_eval['y_pred_class'] = lgbmCV.predict(df_eval[feature_names])\n",
        "    #df['y_pred_class'] = (model_obj.predict_proba(df[feature_names])[:,1] >= 0.5).astype(bool)\n",
        "\n",
        "\n",
        "    #######################################################################\n",
        "    # Calculate Metric\n",
        "    #######################################################################\n",
        "    auc_score = roc_auc_score(y_true = df_eval[target].astype(int), y_score = df_eval['y_pred'])\n",
        "    precision = precision_score(df_eval[target].astype(int), df_eval['y_pred_class'])\n",
        "    recall = recall_score(df_eval[target].astype(int), df_eval['y_pred_class'])\n",
        "\n",
        "    print ('AUC Score:' , auc_score)\n",
        "    print ('Precision:' , precision)\n",
        "    print ('Recall:' , recall )\n",
        "\n",
        "\n",
        "\n",
        "    confusion_matrix = pd.crosstab(df_eval[target], df_eval['y_pred_class'], rownames=['Actual'], colnames=['Predicted'])\n",
        "    #confusion_matrix.to_csv(exp_set + f'train_confusion_matrix.csv')\n",
        "    with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", engine=\"openpyxl\") as writer: \n",
        "        confusion_matrix.to_excel(writer, sheet_name=\"cf\", index=True)\n",
        "\n",
        "\n",
        "    metric_result = pd.DataFrame({'AUC' : [auc_score], 'Precision' : [precision], 'Recall': [recall]})\n",
        "    metric_result.to_csv(exp_set + f'train_metric.csv')\n",
        "\n",
        "    #######################################################################\n",
        "    # Create table\n",
        "    #######################################################################\n",
        "    lenx = len(df_eval)\n",
        "\n",
        "    for qtile,n_ in [('decile',10), ('percentile',lenx)]:\n",
        "        # Add the qtile label to rows\n",
        "        df_eval[target] = df_eval[target].astype('int')\n",
        "        df_eval[qtile] = pd.qcut(df_eval['y_pred'].rank(method='first', ascending=False), n_, labels=False)\n",
        "\n",
        "        # Aggregate rows to qtile bin\n",
        "        agg_df = df_eval.groupby(qtile).agg(\n",
        "            {qtile:len , target:np.nansum, 'y_pred':[np.nanmin, np.nanmax]})\n",
        "\n",
        "        agg_df.columns = ['total_cnt','positives','pred_min', 'pred_max']\n",
        "\n",
        "        agg_df['positive_rate'] = agg_df['positives'] / agg_df['total_cnt']\n",
        "\n",
        "        avg_capture = np.sum(agg_df.positives) / np.sum(agg_df.total_cnt)\n",
        "\n",
        "        agg_df['decile_lift'] = agg_df['positive_rate'] / avg_capture\n",
        "\n",
        "        agg_df.sort_index(ascending = True, inplace = True)\n",
        "\n",
        "        agg_df['cumsum_total_cnt'] = agg_df.total_cnt.cumsum()\n",
        "\n",
        "        agg_df['cumsum_positives'] = agg_df.positives.cumsum()\n",
        "\n",
        "\n",
        "        if qtile == 'decile':\n",
        "            range_size = 10\n",
        "        else:\n",
        "            range_size = lenx\n",
        "\n",
        "\n",
        "        agg_df['naive_rt'] = [(i+1)/range_size for i in range(range_size)]\n",
        "\n",
        "        agg_df['naive_cumsum_positives'] = agg_df.naive_rt * np.sum(agg_df.positives)\n",
        "\n",
        "        agg_df['total_gain'] = agg_df.cumsum_positives / agg_df.naive_cumsum_positives\n",
        "\n",
        "        agg_df['precision_test'] = agg_df.cumsum_positives / agg_df.cumsum_total_cnt \n",
        "\n",
        "        #qtile_dfs[qtile] = agg_df.copy(deep=True)\n",
        "        \n",
        "        with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer: \n",
        "            agg_df.to_excel(writer, sheet_name=f'train_{qtile}', index=True)\n",
        "            # agg_df.to_csv(f'{exp_set}_train_lift_{qtile}.csv')\n",
        "\n",
        "        \n",
        "    #######################################################################\n",
        "    # Print Feature Importance\n",
        "    #######################################################################\n",
        "\n",
        "    feature_impt = pd.DataFrame({'Value':lgbmCV.feature_importances_,'Feature':lgbmCV.feature_name}).sort_values(by = 'Value', ascending = False)\n",
        "    with pd.ExcelWriter(f\"{exp_set}{scenario}.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer: \n",
        "        feature_impt.to_excel(writer, sheet_name=(f'fi_train_{qtile}'), index=True)\n",
        "        #feature_impt.to_csv(exp_set + f'feature_importance_{exp_set}_train_lift_{qtile}.csv')\n",
        "\n",
        "\n",
        "\n",
        "    print (\"Completed\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1690502724718
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}